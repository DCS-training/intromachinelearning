{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Machine Learning\n",
    "## Session 2a: More Classification Models (Decision Trees and k-NN)\n",
    "\n",
    "Welcome to Session 2a! Today, we’ll explore two additional classification models: Decision Trees and k-Nearest Neighbours (k-NN). Each of these models uses different approaches to classify data, which we’ll examine in depth.\n",
    "\n",
    "We’ll apply both models to the Titanic dataset and compare them to our previous Logistic Regression model, looking at accuracy, precision, recall, and model interpretability. By the end, you’ll have a broader toolkit for classification tasks and a better understanding of how different models handle the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing packages and data prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import additional libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess Titanic dataset\n",
    "titanic_data = pd.read_csv(\"../data/titanic_train.csv\")\n",
    "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
    "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female': 1})\n",
    "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Decision Tree Classifier\n",
    "tree_model = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Train the Decision Tree model on X_train and y_train.\n",
    "# Hint: Use .fit() method.\n",
    "\n",
    "tree_model.fit(____, ____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Predict on X_test and calculate accuracy, precision, and recall.\n",
    "\n",
    "y_pred_tree = tree_model.predict(____)\n",
    "\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "precision_tree = precision_score(y_test, y_pred_tree)\n",
    "recall_tree = recall_score(y_test, y_pred_tree)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {accuracy_tree}\")\n",
    "print(f\"Decision Tree Precision: {precision_tree}\")\n",
    "print(f\"Decision Tree Recall: {recall_tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_tree, display_labels=['Did Not Survive', 'Survived']).plot(cmap='Blues')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. k-Nearest Neighbours (k-NN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the k-NN model with 5 neighbours\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Train the k-NN model on X_train and y_train.\n",
    "\n",
    "knn_model.fit(____, ____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Predict on X_test using the k-NN model, then calculate accuracy, precision, and recall.\n",
    "\n",
    "y_pred_knn = knn_model.predict(____)\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "precision_knn = precision_score(y_test, y_pred_knn)\n",
    "recall_knn = recall_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"k-NN Accuracy: {accuracy_knn}\")\n",
    "print(f\"k-NN Precision: {precision_knn}\")\n",
    "print(f\"k-NN Recall: {recall_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for k-NN\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=['Did Not Survive', 'Survived']).plot(cmap='Blues')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a performance comparison table for Decision Tree and k-NN\n",
    "performance = pd.DataFrame({\n",
    "    \"Model\": [\"Decision Tree\", \"k-NN\"],\n",
    "    \"Accuracy\": [accuracy_tree, accuracy_knn],\n",
    "    \"Precision\": [precision_tree, precision_knn],\n",
    "    \"Recall\": [recall_tree, recall_knn]\n",
    "})\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which model has the highest accuracy? \n",
    "2. Do precision and recall tell a different story compared to accuracy?\n",
    "3. Which model do you think would be more reliable for predicting survival? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision Boundaries\n",
    "\n",
    "A decision boundary is a line or surface that separates different classes in a classification model. It helps to visualise how a model decides which class a data point belongs to based on its features. Imagine you’re classifying passengers on the Titanic as either “survived” or “did not survive” based on characteristics like ticket class and fare paid. A decision boundary shows where the model would classify one group differently from another. If a new data point falls on one side of the boundary, it gets classified into one category; if it falls on the other, it’s assigned to the other category.\n",
    "\n",
    "Different models create decision boundaries in different ways. For example, Decision Trees tend to create straight, box-like boundaries as they split the data sequentially by feature values, which can make them appear less smooth but highly interpretable. k-Nearest Neighbours (k-NN), on the other hand, classifies points based on the nearest neighbours around them, so the decision boundary is influenced by the actual data distribution. This can result in more complex and flexible boundaries that adapt to the structure of the data, especially when more neighbours are considered.\n",
    "\n",
    "Decision boundaries are a helpful visual tool for understanding a model’s behaviour and limitations. In situations with clear separation between classes, decision boundaries help us see where the model might make mistakes or misclassify points, especially when classes overlap. By plotting these boundaries, we can better understand why a model may make certain predictions and where it might struggle with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting two features for visualising decision boundaries (Pclass and Fare)\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    X_selected = X[['Pclass', 'Fare']].values\n",
    "    model.fit(X_selected, y)\n",
    "\n",
    "    x_min, x_max = X_selected[:, 0].min() - 1, X_selected[:, 0].max() + 1\n",
    "    y_min, y_max = X_selected[:, 1].min() - 1, X_selected[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))\n",
    "    plt.scatter(X_selected[:, 0], X_selected[:, 1], c=y, edgecolor='k', s=20, cmap=ListedColormap(['#FF0000', '#0000FF']))\n",
    "    plt.xlabel(\"Pclass\")\n",
    "    plt.ylabel(\"Fare\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for Decision Tree\n",
    "plot_decision_boundary(tree_model, X_train, y_train, \"Decision Tree Decision Boundary\")\n",
    "\n",
    "# Plot decision boundary for k-NN\n",
    "plot_decision_boundary(knn_model, X_train, y_train, \"k-NN Decision Boundary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
